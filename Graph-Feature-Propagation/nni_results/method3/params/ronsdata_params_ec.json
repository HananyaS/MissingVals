{
  "batch_size": 32,
  "lr": 0.01,
  "preweight": 5,
  "layer_1": 12,
  "layer_2": 7,
  "activation": "relu",
  "dropout": 0.4
}